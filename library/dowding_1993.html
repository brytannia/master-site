<!DOCTYPE HTML>
<!--
	TXT 2.5 by HTML5 UP
	html5up.net | @n33co
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>John Dowding and others - Gemini: A Natural Language System For Spoken-Language Understanding</title>
		<meta http-equiv="content-type" content="text/html; charset=utf-8" />
		<link href="http://fonts.googleapis.com/css?family=PT+Sans:400,700" rel="stylesheet" />

		<script src="../js/config_sub.js"></script>
		<script src="../js/skel.min.js"></script>
		<noscript>
			<link rel="stylesheet" href="../inxex/css/skel-noscript.css" />
			<link rel="stylesheet" href="../../css/style.css" />
			<link rel="stylesheet" href="../../css/style-desktop.css" />
		</noscript>
	</head>
	<body class="homepage">
	<div class="index">
			<a href="index.html" class="index">Назад в библиотеку</a>
	</div>

		
		
		
		<!-- Main -->
			<div id="main-wrapper" style="border-top-width: 0px">
				<div id="main" class="container">
					<div class="row">
						<div class="12u">

							<!-- Features -->
								<section class="is-features">
									<h3 class="major"><span>Gemini: A Natural Language System For Spoken-Language Understanding</h3>
									<article class="header">
														<span class="titler">Авторы: </span><span>John Dowding, Jean Mark Gawron, Douglas E. Appelt, John Bear, Lynn Cherny, Robert C. Moore , Douglas Moran</br>
														<span class="titler">Источник: </span> John Dowding, Jean Mark Gawron, Doug Appelt,
John Bear, Lyn Cherny, Robert Moore, and Doug
Moran. Gemini: A natural language system for
spoken-language understanding. In Proceedings of
the DARPA Speech and Natural Language Workshop, March 1993. 54-60 pp.</span>
									</article>
									<div>
										
										<section>
										<h3 class="article">1. INTRODUCTION 
</h3>
										<p>Gemini is a natural language (NL) under
standing system developed for spoken language 
applications. This paper describes the details of 
the system, and includes relevant measurements 
of size, efficiency, and performance of each of its 
components.</p><p>
In designing any NL understanding system, 
there is a tension between robustness and correct
ness. Forgiving an error risks throwing away cru
cial information; furthermore, devices added to a 
system to enhance robustness can sometimes en
rich the ways of finding an analysis, multiplying 
the number of analyses for a given input, and mak
ing it more difficult to find the correct analysis. In 
processing spoken language this tension is height
ened because the task of speech recognition in
troduces a new source of error. The robust sys
tem will attempt to find a sensible interpretation, 
even in the presence of performance errors by the 
speaker, or recognition errors by the speech rec
ognizer. On the other hand, a system should be 
able to detect that a recognized string is not a sen
tence of English, to help filter recognition errors by 
the speech recognizer. Furthermore, if parsing and 
recognition are interleaved, then the parser should 
enforce constraints on partial utterances. 
The approach taken in Gemini is to con
strain language recognition with fairly conven
tional grammar, but to augment that grammar 
with two orthogonal rule-based recognition mod
ules, one for glueing together the fragments found 
during the conventional grammar parsing phase, 
and another for recognizing and eliminating dis
fluencies known as "repairs." At the same time, the multiple analyses arising before and after all 
this added robustness are managed in two ways: 
first, by highly constraining the additional rule
based modules by partitioning the rules into pref
erence classes, and second, through the addition 
of a postprocessing parse preference component. 
Processing starts in Gemini when syntac
tic, semantic, and lexical rules are applied by a 
bottom-up all-paths constituent parser to populate 
a chart with edges containing syntactic, seman
tic, and logical form information. Then, a second 
utterance parser is used to apply a second set of 
syntactic and semantic rules that are required to 
span the entire utterance. If no semantically ac
ceptable utterance-spanning edges are found dur
ing this phase, a component to recognize and cor
rect certain grammatical disfluencies is applied. 
When an acceptable interpretation is found, a set 
of parse preferences is used to choose a single best 
interpretation from the chart to be used for sub
sequent processing. Quantifier scoping rules are 
applied to this best interpretation to produce the 
final logical form, which is then used as input to 
a query-answering system. The following sections 
describe each of these components in detail, with 
the exception of the query-answering subsystem, 
which is not described in this paper. 
In our component-by-component view of 
Gemini, we provide detailed statistics on each 
component's size, speed, coverage, and accuracy. 
These numbers detail our performance on the sub
domain of air-travel planning that is currently be
ing used by the ARPA spoken language under
standing community (MADCOW, 1992). Gem
ini was trained on a 5875-utterance dataset from 
this domain, with another 688 utterances used as 
a blind test (not explicitly trained on, but run 
nmltiple times) to monitor our performance on a 
dataset on which we did not train. We also report 
here our results on another 756-utterance fair test 
set that we ran only once. Table 1 contains a sum
mary of the coverage of the various components on 
both the training and fair test sets. More detailed explanations of these numbers are given in the rel
evant sections. 
</p>
<h3 class="article">2. SYSTEM DESCRIPTION 
</h3>
										<p>Gemini maintains a firm separation between 
the languageand domain-specific portions of the 
system, and the underlying infrastructure and ex
ecution strategies. The Gemini kernel consists of 
a set of compilers to interpret the high-level lan
guages in which the lexicon and syntactic and se
mantic grammar rules are written, as well as the 
parser, semantic interpretation, quantifier scop
ing, repair correction mechanisms, and all other 
aspects of Gemini that are not specific to a lan
guage or domain. Although this paper describes 
the lexicon, grammar, and semantics of English, 
Gemini has also been used in a Japanese spo
ken language understanding system (Kameyama, 
1992).
</p>

<h3 class="article">2.1. Grammar Formalism 

</h3>
										<p>Gemini includes a midsized constituent gram
mar of English (described in section 2.3), a small 
utterance grammar for assembling constituents 
into utterances (described in section 2.7), and a 
lexicon. All three are written in a variant of the 
unification formalism used in the Core Language 
Engine (Alshawi, 1992) . </p><p>
The basic building block of the grammar for
malism is a category with feature constraints. 
Here is an example: </p><p>
np: [wh=ynq, case= (nomVacc), 
pers_num= (3rdAsg) ] </p><p>
This category can be instantiated by any noun 
phrase with the value ynq for its wh feature (which 
means it must be a wh-bearing noun phrase like 
which book, who, or whose mother), either ace (ac
cusative) or nora (nominative) for its case feature, 
and the conjunctive value 3rdAsg (third and sin
gular) for its person-number feature. This for
malism is related directly to the Core Language 
Engine, but more conceptually it is closely re
lated to that of other unification-based grammar 
formalisms with a context-free skeleton, such as 
PATR-II (Shieber et al., 1983), Categorial Uni
fication Grammar (Uszkoreit, 1986), Generalized 
Phrase-Structure Grammar (Gazdar et al., 1982), 
and Lexical Functional Grammar (Bresnan, 1982). 
Gemini differs from other unification for
malisms in the following ways. Since many of 
the most interesting issues regarding the formal
ism concern typing, we defer discussing motivation 
until section 2.5. </p><p>
Gemini uses typed unification. Each category 
has a set of features declared for it. Each fea
ture has a declared value space of possible values 
(value spaces may be shared by different fea
tures). Feature structures in Gemini can be re
cursive, but only by having categories in their 
value space; so typing is also recursive. Typed 
feature structures are also used in HPSG (Pol
lard and Sag, in press). One important differ
ence with the use in Gemini is that Gemini has 
no type inheritance. </p><p>
Some approaches do not assume a syntactic 
skeleton of category-introducing rules (for ex
ample, Functional Unification Grammar (Kay, 
1979)). Some make such rules implicit (for 
example, the various categorial unification ap
proaches, such as Unification Categorial Gram
mar (Zeevat, Klein, and Calder, 1987)). 
Even when a syntactic skeleton is assumed, 
some approaches do not distinguish the category 
of a constituent (for example, rip, vp) from its 
other features (for example, pers_aum, gapsin, 
gapsout). Thus, for example, in one version of 
GPSG, categories were simply feature bundles 
(attribute value structures) and there was a fea
ture l~hJ taking values like N,V,A, and P which 
determined the major category of constituent. 
• Gemini does not allow rules schematizing over 
syntactic categories. 

</p>

<h3 class="article">2.2. Lexicon 
</h3>
										<p>The Gemini lexicon uses the same category 
notation as the Gemini syntactic rules. Lexical 
categories are types as well, with sets of features 
defined for them. The lexical component of Gem
ini includes the lexicon of base forms, lexical tem
plates, morphological rules, and the lexical type 
and feature default specifications. </p><p>
The Gemini lexicon used for the air-travel 
planning domain contains 1,315 base entries. 
These expand by morphological rules to 2,019. In 
the 5875-utterance training set, 52 sentences con
tained unknown words (0.9%), compared to 31 
sentences in the 756-utterance fair test set (4.1%). 
</p>
<h3 class="article">2.3. Constituent Grammar 
</h3>
										<p>A simplified example of a syntactic rule is 
</br>
syn (whq_ynq_slash_np, </br>
[ s: [sentence_type=whq, form=tnsd, </br>
gapsin=G, gapsout=G], </br>
np: [wh=ynq, pers_num=N] , </br>
s : [sentence_type=ynq, form=tnsd, </br>
gapsin=np: [pets_hum=N], </br>
gapsout =null] ] ). 
</br>
This syntax rule (named whq_ynq..$1ash..np) 
says that a sentence (category s) can be built by 
finding a noun phrase (category np) followed by a 
sentence. It requires that the daughter np have the 
value ynq for its wh feature and that it have the 
value 1~ (a variable) for its person-number feature. 
It requires that the daughter sentence have a cat
egory value for its gapsin feature, namely an np 
with a person number value N, which is the same as 
the person number value on the wh-bearing noun 
phrase. The interpretation of the entire rule is 
that a gapless sentence with sentence_type whq 
can be built by finding a wh-phrase followed by a 
sentence with a noun phrase gap in it that has the 
same person number as the wh-phrase.  </p><p>
Semantic rules are written in much the same 
rule format, except that in a semantic rule, each of 
the constituents mentioned in the phrase structure 
skeleton is associated with a logical form. Thus, 
the semantics for the rule above is 
</br>
s em (whq_ynq_slash_np, 
[( [,hq, S], s : ['1 ), 
(Np, np: []), 
(S, s : [gapsin=np: [gapsem=Np] ] )] ). 
</br>
Here the semantics of the mother s is just the 
semantics of the daughter s with the illocution
ary force marker whq wrapped around it. In addi
tion, the semantics of the s gap's np's gapsem has 
been unified with the semantics of the wh-phrase. 
Through a succession of unifications this will end 
up assigning the wh-phrase's semantics to the gap 
position in the argument structure of the s. Al
though each semantic rule must be keyed to a pre
existing syntactic rule, there is no assumption of 
rule-to-rule uniqueness. Any number of semantic 
rules may be written for a single syntactic rule. 
We discuss some further details of the semantics 
in section 2.6  </p><p>
The constituent grammar used in Gemini con
tains 243 syntactic rules, and 315 semantic rules. 
Syntactic coverage on the 5875-utterance training 
set was 94.2%, and on the 756-utterance test set 
it was 90.9%. 
</p>

<h3 class="article">2.4. Parser 
</h3>
										<p>Since Gemini was designed with spoken lan
guage interpretation in mind, key aspects of the 
Gemini parser are motivated by the increased 
needs for robustness and efficiency that charac
terize spoken language. Gemini uses essentially a pure bottom-up chart parser, with some limited 
left-context constraints applied to control creation 
of categories containing syntactic gaps.  </p><p>
Some key properties of the parser are  </p><p>
• The parser is all-paths bottom-up, so that all 
possible edges admissible by the grammar are 
found.  </p><p>
• The parser uses subsumption checking to reduce 
the size of the chart. Essentially, an edge is not 
added to the chart if it is less general than a 
preexisting edge, and preexisting edges are re
moved from the chart if the new edge is more 
general.  </p><p>
• The parser is on-line (Graham, Harrison, and 
Russo, 1980), essentially meaning that all edges 
that end at position i are constructed before 
any that end at position i + 1. This feature is 
particularly desirable if the final architecture of 
the speech understanding system couples Gem
ini tightly with the speech recognizer, since it 
guarantees for any partial recognition input that 
all possible constituents will be built. 
An important feature of the parser is the 
mechanism used to constrain the construction of 
categories containing syntactic gaps. In earlier 
work (Moore and Dowding, 1991), we showed that 
approximately 80% of the edges built in an all
paths bottom-up parser contained gaps, and that 
it is possible to use prediction in a bottom-up 
parser only to constrain the gap categories, with
out requiring prediction for nongapped categories. 
This limited form of left-context constraint greatly 
reduces the total number of edges built for a very 
low overhead. In the 5875-utterance training set, 
the chart for the average sentence contained 313 
edges, but only 23 predictions. 
</p>
<h3 class="article">2.5. Typing 
</h3>
<p>The main advantage of typed unification is for 
grammar development. The type information on 
features allows the lexicon, grammar, and seman
tics compilers to provide detailed error analysis re
garding the flow of values through the grammar, 
and to warn if features are assigned improper val
ues, or variables of incompatible types are unified. 
Since the type-analysis is performed statically at 
compile time, there is no run-time overhead asso
ciated with adding types to the grammar.  </p><p>
The major grammatical category plays a spe
cial role in the typing scheme of Gemini. For each 
category, Gemini makes a set of declarations stipu
lating its allowable features and the relevant value 
spaces. Thus, the distinction between the syntac
tic category of a constituent and its other features 
can be cashed out as follows: the syntactic cat
egory can be thought of as the feature structure type. The only other types needed by Gemini are 
the value spaces used by features. Thus for ex
ample, the type v (verb) admits a feature vforra, 
whose value space vforra-types call be instanti
ated with values like present participle, finite, and 
past participle. Since all recursive features are 
category-valued, these two kinds of types suffice. 
</p>
<h3 class="article">2.6. Interleaving Syntactic and Se
mantic Information 
</h3>
<p>
Sortal Constraints Selectional restrictions 
are imposed in Gemini through the sorts mecha
nism. Selectional restrictions include both highly 
domain-specific information about predicate
argument and very general predicate restrictions. 
For example, in our application the object of 
the transitive verb depart (as in flights departing 
Boston) is restricted to be an airport or a city, 
obviously a domain-specific requirement. But the 
same machinery also restricts a determiner like all 
to take two propositions, and an adjective like fur
ther to take distances as its measure-specifier (as 
in thirty miles further). In fact, sortal constraints 
are assigned to every atomic predicate and opera
tor appearing in the logical forms constructed by 
the semantic rules.  </p><p>
Sorts are located in a conceptual hierarchy 
and are implemented as Prolog terms such that 
more general sorts subsume more specific sorts 
(Mellish, 1988). This allows the subsumption 
checking and packing in the parser to share struc
ture whenever possible. Semantic coverage with 
sortal constraints applied was 87.4% on the train
ing set, and on the test set it was 83.7%. 
Interleaving Semantics with Parsing In 
Gemini, syntactic and semantic processing is fully 
interleaved. Building an edge requires that syntac
tic constraints be applied, which results in a tree 
structure, to which semantic rules can be applied, 
which results in a logical form to which sortal con
traints can be applied. Only if the syntactic edge 
leads to a well-sorted semantically-acceptable log
ical form fragment is it added to the chart. 
Interleaving the syntax and semantics in this 
way depends on a crucial property of the seman
tics: a semantic interpretation is available for each 
syntactic node. This is guaranteed by the seman
tic rule formalism and by the fact that every lexical 
item has a semantics associated with it. 
</p>
<h3 class="article">2.7. Utterance Parsing 
</h3>
<p>The constituent parser uses the constituent 
grammar to build all possible categories bottom
up, independent of location within the string. 
Thus, the constituent parser does not force any 
constituent to occur either at the beginning of 
the utterance, or at the end. Those constraints 
are stated in what we call the utterance grammar. 
They are applied after constituent parsing is com
plete by the utterance parser. The utterance gram
mar specifies ways of combining the categories 
found by the constituent parser into an analysis 
of the complete utterance. It is at this point that 
the system recognizes whether the sentence was 
a simple complete sentence, an isolated sentence 
fragment, a run-on sentence, or a sequence of re
lated fragments.  </p><p>
Many systems (Carbonell and Hayes, 1983), 
(Hobbs et al., 1992), (Seneff, 1992), (Stallard and 
Bobrow, 1992) have added robustness with a sim
ilar postprocessing phase. The approach taken 
in Gemini differs in that the utterance grammar 
uses the same syntactic and semantic rule for
malism used by the constituent grammar. Thus, 
the same kinds of logical forms built during con
stituent parsing are the output of utterance pars
ing, with the same sortal constraints enforced. For 
example, an utterance consisting of a sequence 
of modifier fragments (like on Tuesday at three 
o'clock on United) is interpreted as a conjoined 
property of a flight, because the only sort of thing 
in the ATIS domain that can be on Tuesday at 
three o'clock on United is a flight.  </p><p>
The utterance parser partitions the utterance 
grammar into equivalence classes and considers 
each class according to an ordering. Utterance 
parsing terminates when all constituents satisfy
ing the rules of the current equivalence class are 
built, unless there are none, in which case the next 
class is considered. The highest ranked class con
sists of rules to identify simple complete sentences, 
the next highest class consists of rules to iden
tify simple isolated sentence fragments, and so on. 
Thus, the utterance parser allows us to enforce a 
very coarse form of parse preferences (for exam
ple, prefering complete sentences to sentence frag
ments). These coarse preferences could also be 
enforced by the parse preference component de scribed in section 2.9, but for the sake of efficiency 
we choose to enforce them here. 
The utterance grammar is significantly 
smaller than the constituent grammar only 37 
syntactic rules and 43 semantic rules. 
</p>
<h3 class="article">2.8. Repairs 
</h3>
<p>
Grammatical disfluencies occur frequently in 
spontaneous spoken language. We have imple
mented a component to detect and correct a large 
subclass of these disfluencies (called repairs, or 
self-corrections) where the speaker intends that 
the meaning of the utterance be gotten by deleting 
one or more words. Often, the speaker gives clues 
of their intention by repeating words or adding cue 
words that signal the repair:  </p><p>
(1) a. How many American airline flights leave 
Denver on June June tenth.  </p><p>
b. Can you give me information on all the 
flights from San Francisco no from Pitts
burgh to San Francisco on Monday.  </p><p>
The mechanism used in Gemini to detect and 
correct repairs is currently applied as a fallback if 
no semantically acceptable interpretation is found 
for the complete utterance. The mechanism finds 
sequences of identical or related words, possibly 
separated by a cue word (for example, oh or no) 
that might indicate the presence of a repair, and 
deletes the first occurrence of the matching por
tion. Since there may be several such sequences of 
possible repairs in the utterance, the mechanism 
produces a ranked set of candidate corrected ut
terances. These candidates are ranked in order 
of the fewest deleted words. The first candidate 
that can be given an interpretation is accepted as 
the intended meaning of the utterance. This ap
proach is presented in detail in (Bear, Dowding, 
and Shriberg, 1992).  </p><p>
The repair correction mechanism helps in
crease the syntactic and semantic coverage of 
Gemini (as reported in Table 1). In the 5875
utterance training set, 178 sentences contained 
nontrivial repairs 2, of which Gemini found 89 
(50%). Of the sentences Gemini corrected, 81 were 
analyzed correctly (91%), and 8 contained repairs 
but were corrected wrongly. Similarly, the 756
utterance test set contained 26 repairs, of which 
Gemini found 11 (42%). Of those 11, 8 were ana
lyzed correctly (77%), and 3 were analyzed incor
rectly.  </p><p>
Since Gemini's approach is to extend lan
guage analysis to recognize specific patterns char
acteristic of spoken language, it is important for components like repair correction (which provide 
the powerful capability of deleting words) not to 
be applied in circumstances where no repair is 
present. In the 5875-utterance training set, Gem
ini misidentified only 15 sentences (0.25%) as con
taining repairs when they did not. In the 756
utterance test set, only 2 sentences were misiden
tiffed as containing repairs (0.26%). 
While the repair correction component cur
rently used in Gemini does not make use of acous
tic/prosodic information, it is clear that acoustics 
can contribute meaningful cues to repair. In fu
ture work, we hope to improve the performance of 
our repair correction component by incorporating 
acoustic/prosodic techniques for repair detection 
(Bear, Dowding, and Shriberg, 1992) (Nakatani 
and Hirschberg, 1993) (O'Shaughnessy, 1992). 
A central question about the repairs module 
concerns its role in a tightly integrated system in 
which the NL component filters speech recognition 
hypotheses. The open question: should the repairs 
module be part of the recognizer filter or should 
it continue to be a post-processing component? 
The argument for including it in the filter is that 
without a repairs module, the NL system rejects 
many sentences with repairs, and will thus dispre
fer essentially correct recognizer hypotheses. The 
argument against including it is efficiency and the 
concern that with recognizer errors present, the 
repair module's precision may suffer: it may at
tempt to repair sentences with no repair in them. 
Our current best guess is that recognizer errors 
are essentially orthogonal to repairs and that a 
filter including the repairs module will not suffer 
from precision problems. But we have not yet per
formed the experiments to decide this. 
</p>
<h3 class="article">2.9. Parse Preference Mechanism 
</h3>
<p>In Gemini, parse preferences are enforced 
when extracting syntactically and semantically 
well-formed parse trees from the chart. In this 
respect, our approach differs from many other 
approaches to the problem of parse preferences, 
which make their preference decisions as pars
ing progresses, pruning subsequent parsing paths 
(Frazier and Fodor, 1978), (Hobbs and Bear, 
1990), (Marcus 1980). Applying parse prefer
ences requires comparing two subtrees spanning 
the same portion of the utterance.  </p><p>
The parse preference mechanism begins with 
a simple strategy to disprefer parse trees contain
ing specific "marked" syntax rules. As an example 
of a dispreferred rule, consider: Book those three 
flights to Boston. This sentence has a parse on 
which those three is a noun phrase with a miss
ing head (consider a continuation of the discourse 
Three of our clients have sufficient credit). After 
penalizing such dispreferred parses, the preference mechanism applies attachment heuristics based on 
the work by Pereira (1985) and Shieber (1983) 
Pereira's paper shows how the heuristics of 
Minimal Attachment and Right Association (Kim
ball, 1973) can both be implemented using a 
bottom-up shift-reduce parser.  </p><p>
(2)(a) John sang a song for Mary.  </p><p>
(b) John canceled the room Mary reserved yes
terday.  </p><p>
Minimal Attachment selects for the tree with the 
fewest nodes, so in (2a), the parse that makes for 
Mary a complement of sings is preferred. Right 
Association selects for the tree that incorporates 
a constituent A into the rightmost possible con
stituent (where rightmost here means beginning 
the furthest to the right). Thus, in (2b) the parse 
in which yesterday modifies reserved is preferred. 
The problem with these heuristics is that 
when they are formulated loosely, as in the pre
vious paragraph, they appear to conflict. In par
ticular, in (2a), Right Association seems to call for 
the parse that makes for Mary a modifier of song. 
Pereira's goal is to show how a shift-reduce 
parser can enforce both heuristics without conflict 
and enforce the desired preferences for examples 
like (2a) and (2b). He argues that Minimal At
tachment and Right Association can be enforced in 
the desired way by adopting the following heuris
tics for resolving conflicts:  </p><p>
1. Right Association: In a shift-reduce conflict, 
prefer shifts to reduces.  </p><p>
2. Minimal Attachment: In a reduce-reduce con
flict, prefer longer reduces to shorter reduces. 
Since these two principles never apply to the same 
choice, they never conflict.  </p><p>
For purposes of invoking Pereira's heuristics, 
the derivation of a parse can be represented as the 
sequence of S's (Shift) and R's (Reduce) needed to 
construct the parse's unlabeled bracketing. Con
sider, for example, the choice between two unla
beled bracketings of (2a):  </p><p>
(a) [John [sang [a song ] [for Mary ] ] ] </br>
S S S S R S S RRR </br>
(b) [John [sang [[a song] [for Mary ]] ]] </br>
S S S S R S S RRRR </br>
There is a shift for each word and a reduce for 
each right bracket. Comparison of the two parses 
consists simply of pairing the moves in the shift
reduce derivation from left to right. Any parse 
making a shift move that corresponds to a reduce 
move loses by Right Association. Any parse mak
ing a reduce move that corresponds to a longer 
reduce loses by Minimal Attachment. In deriva
tion (b) above, the third reduce move builds the constituent a song for Mary from two constituents, 
while the corresponding reduce in (a) builds sang 
a song for Mary from three constituents. Parse 
(b) thus loses by Minimal Attachment. 
Questions about the exact nature of parse 
preferences (and thus about the empirical ade
quacy of Pereira's proposal) still remain open, but 
the mechanism sketched does provide plausible re
sults for a number of examples. 
</p>
<h3 class="article">2.10. Scoping</h3>
<p>The final logical form produced by Gemini 
is the result of applying a set of quantifier scop
ing rules to the best interpretation chosen by the 
parse preference mechanism. The semantic rules 
build quasi-logical forms, which contain complete 
semantic predicate-argument structure, but do not 
specify quantifier scoping. The scoping algorithm 
that we use combines syntactic and semantic in
formation with a set of quantifier scoping prefer
ence rules to rank the possible scoped logical forms 
consistent with the quasi-logical form selected by 
parse preferences. This algorithm is described in 
detail in (Moran, 1988). 
</p>
<h3 class="article">3. CONCLUSION 
</h3>
<p>
In our approach to resolving the tension be
tween overgeneration and robustness in a spoken 
language understanding system, some aspects of 
Gemini are specifically oriented towards limiting 
overgeneration, such as the on-line property for 
the parser, and fully interleaved syntactic and se
mantic processing. Other components, such as the 
fragment and run-on processing provided by the 
utterance grammar, and the correction of recog
nizable grammatical repairs, increase the robust
ness of Gemini. We believe a robust system can 
still recognize and disprefer utterances containing 
recognition errors. </p><p>
Research in the construction of the Gemini 
system is ongoing to improve Gemini's speed and 
coverage, as well as to examine deeper integration 
strategies with speech recognition, and integration 
of prosodic information into spoken !anguage dis
ambiguation.
</p>
<h3 class="article">REFERENCES 
</h3>
<p>
[1] Alshawi, tI. (ed) (1992). The Core Language En
gine, MIT Press, Cambridge. 
</br>
[2]Bear, J., Dowding, J., and Shriberg, E. (1992). 
"Integrating Multiple Knowledge Sources for 
the Detection and Correction of Repairs in 
Human-Computer Dialog", in Proceedings of 
the 30lh Annual Meeting of the Association 
for Computational Linguists, Newark, DE,pp. 
56-63. 
</br>
[3]Bresnan, ,]. (ed) (1982). The Mental Represen
tation of Grammatical Relations, MIT Press, 
Cambridge</br>
[4]Carbonell, J., and Hayes, P. (1983). "Recovery 
Strategies for Parsing Extragrammatical Lan
guage", American Journal of Computational 
Linguistics, Vol. 9, Numbers 3-4, pp. 123-146.</br>
for Spoken Language Identiﬁcation, 2006
[5]Frazier, L., and Fodor, J.D. (1978). "The Sausage 
Machine: A New Two-Stage Parsing Model", 
Cognition, Vol. 6, pp. 291-325.</br>
[6]Gazdar, G., Klein, E., Pullum, G., and Sag, I. 
(1982). Generalized Phrase Structure Gram
mar, Harvard University Press, Cambridge. 
</br>
[7]Graham, S., ttarrison, M., and Ruzzo, W. 
(1980). "An Improved Context-Free Recog
nizer", ACM Transactions on Programming 
Languages and Systems, Vol. 2, No. 3, pp. 415
462.</br>
[8]Hobbs, J., and Bear, J. (1990). "Two Principles 
of Parse Preference", in Proceedings of the 
13th International Conference on Computa
tional Linguistics, Helsinki, Vol. 3, pp. 162
167.</br>
[9]Hobbs, J., Appelt, D., Bear, J., Tyson, M., and 
Magerman, D. (1992). "Robust Processing 
of Real-World Natural-Language Texts", in 
Text Based Intelligent Systems, ed. P. Jacobs, 
Lawrence Erlbaum Associates, Hillsdale, N J, 
pp. 13-33.
</p>
													</section>
												<!-- /Feature -->
										
									</div>
								</section>
							<!-- /Features -->

						</div>
					</div>
					
					</div>
				</div>
			</div>
		<!-- /Main -->

		<!-- Footer -->
			<footer id="footer" class="container">
				
				<!-- Links -->

					<div > 
					<span>
						<a href="http://donntu.edu.ua/index.php?lang=ru">Официальный сайт ДонНТУ</a> | 
						<a href="http://masters.donntu.edu.ua/index.htm">Портал магистров ДонНТУ</a> 
					</div>

				<!-- /Links>
				
				
				<!-- Copyright -->
					<div id="copyright">
						&copy; Татьяна Брынза | Дизайн: <a href="http://html5up.net/">HTML5 UP</a>
					</div>
				<!-- /Copyright -->

			</footer>
		<!-- /Footer -->

	</body>
</html>